{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, path, input_features, output_features):\n",
    "        self.path = path\n",
    "        self.inp = input_features\n",
    "        self.out = output_features\n",
    "\n",
    "    def extract(self):\n",
    "        df = pd.read_csv(self.path)\n",
    "        X = df[self.inp]\n",
    "        y = df[self.out]\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.35, shuffle=False)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.15, shuffle=False)\n",
    "        X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "        X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "        y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "        X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "        y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "        return X_train, X_test, X_val, y_train, y_test, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = ['GaH_V', 'GaL_V', 'GbH_V', 'GbL_V', 'GcH_V', 'GcL_V', 'speed_radps'] #['mAngle_rad'] # ,\n",
    "output_columns = ['Ia_amps', 'Ib_amps', 'Ic_amps']\n",
    "data = Data('PMSM.csv', input_columns, output_columns)\n",
    "train_in, test_in, val_in, train_out, test_out, val_out = data.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_last_column, _ = torch.max(train_in[:, 6], dim=0)\n",
    "train_in[:, 6] /= max_value_last_column.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_conti = nn.Sequential(\n",
    "              nn.Conv1d(1, 256, kernel_size=3, padding=1),\n",
    "              nn.Tanh(),\n",
    "              nn.Conv1d(256, 1024, kernel_size=3, padding=1),\n",
    "              nn.ReLU()\n",
    "          )\n",
    "        self.model_bin = nn.Sequential(\n",
    "              nn.Linear(6, 1024),\n",
    "              nn.ReLU()\n",
    "          )\n",
    "        self.out = nn.Linear(2048, 3)\n",
    "\n",
    "    def forward(self, x_bin, x_conti):\n",
    "        conti_preds = self.model_conti(x_conti)\n",
    "        bin_preds = self.model_bin(x_bin)\n",
    "        comb = torch.cat((conti_preds.squeeze(2), bin_preds), dim=1)\n",
    "        out = self.out(comb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Motor(nn.Module):\n",
    "    def __init__(self, train_in, train_out, val_in, val_out, test_in, test_out, criterion, batch_size):\n",
    "        super(Motor, self).__init__()\n",
    "        dataset = TensorDataset(train_in[:, :6], train_in[:, -1].unsqueeze(1), train_out)\n",
    "        train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        # dataset = TensorDataset(val_in, val_out)\n",
    "        val_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.trainloader = train_dataloader\n",
    "        self.valloader = val_dataloader\n",
    "        # self.val_in = val_in\n",
    "        # self.val_out = val_out\n",
    "        self.test_in = test_in\n",
    "        self.test_out = test_out\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train(self, epochs, optimizer, model):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            for X_bin, X_conti, y_batch in self.trainloader:\n",
    "              optimizer.zero_grad()\n",
    "              # predictions = model(X_batch)\n",
    "              predictions = model(X_bin, X_conti.unsqueeze(1))\n",
    "              loss = self.criterion(predictions, y_batch)\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "              train_loss += loss.item()\n",
    "              # if(epoch==4):\n",
    "              #   print(predictions)\n",
    "              #   print(y_batch)\n",
    "            # preds = model(self.train_in)\n",
    "            # train_loss = torch.sum(self.criterion(preds, self.train_out))\n",
    "            print(f'Epoch{epoch}: Train loss = {train_loss/len(self.trainloader)}')\n",
    "        return model\n",
    "\n",
    "    def test(self, model):\n",
    "        preds = model(self.test_in)\n",
    "        test_loss = torch.sum(self.criterion(preds, self.test_out))\n",
    "        print(f'Test loss = {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: Train loss = 1.570969114453611\n",
      "Epoch1: Train loss = 1.6501222185680062\n",
      "Epoch2: Train loss = 1.7098595175365674\n",
      "Epoch3: Train loss = 1.8204793953539546\n",
      "Epoch4: Train loss = 1.7375896686385994\n",
      "Epoch5: Train loss = 1.830011630370834\n",
      "Epoch6: Train loss = 1.7247578925355496\n",
      "Epoch7: Train loss = 1.838897946892399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srinivasan/Cummins-Project/motor_sim.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m Model()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m motor\u001b[39m.\u001b[39;49mtrain(\u001b[39m10\u001b[39;49m, optimizer, model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# motor.test(model)\u001b[39;00m\n",
      "\u001b[1;32m/home/srinivasan/Cummins-Project/motor_sim.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(predictions, y_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   \u001b[39m# if(epoch==4):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   \u001b[39m#   print(predictions)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   \u001b[39m#   print(y_batch)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# preds = model(self.train_in)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinivasan/Cummins-Project/motor_sim.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# train_loss = torch.sum(self.criterion(preds, self.train_out))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/optim/adam.py:373\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    370\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 373\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39;49madd(param, alpha\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(param):\n\u001b[1;32m    376\u001b[0m     grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "motor = Motor(train_in, train_out, val_in, val_out, test_in, test_out, criterion, 32)\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.001, weight_decay=1e-5)\n",
    "model = motor.train(10, optimizer, model)\n",
    "# motor.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict control signals and angle, predict currents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_last_column, _ = torch.max(test_in[:, 6], dim=0)\n",
    "test_in[:, 6] /= max_value_last_column.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(test_in[0][:6].unsqueeze(0), test_in[0][6].unsqueeze(0).unsqueeze(0).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 318.8877,  292.7519, -661.6600]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 246.8394, -281.8730,   35.0336])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = test_out[0]\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srinivasan/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(273588.9062, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss()(pred, o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
